{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "import gensim\n",
        "\n",
        "# Load your sentimental analysis dataset from assignment three\n",
        "# Replace 'your_dataset.csv' with the actual path or filename\n",
        "# Assuming the dataset has a column 'review' for the text data and 'sentiment' for sentiment labels\n",
        "data = pd.read_csv('/content/Sentimental_analysed_dataset.csv')\n",
        "\n",
        "# (1) Features (text representation) used for topic modeling\n",
        "# Using CountVectorizer to create a document-term matrix\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
        "X = vectorizer.fit_transform(data['Review'])\n",
        "\n",
        "# Convert the document-term matrix to a Gensim corpus\n",
        "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "# Create a dictionary mapping words to their integer ids\n",
        "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
        "\n",
        "# (2) Top 10 clusters for topic modeling\n",
        "# Fit LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
        "\n",
        "# (3) Summarize and describe the topic for each cluster\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6v0OpTSFsPh",
        "outputId": "2e76f39e-be5b-4871-f505-4b801326df7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.020*\"movie\" + 0.020*\"story\" + 0.013*\"hope\" + 0.010*\"andy\" + 0.010*\"red\"')\n",
            "(1, '0.018*\"movie\" + 0.013*\"sway\" + 0.013*\"let\" + 0.010*\"film\" + 0.008*\"didn\"')\n",
            "(2, '0.001*\"film\" + 0.001*\"prison\" + 0.001*\"andy\" + 0.001*\"shawshank\" + 0.001*\"redemption\"')\n",
            "(3, '0.033*\"movie\" + 0.013*\"time\" + 0.012*\"just\" + 0.012*\"great\" + 0.010*\"film\"')\n",
            "(4, '0.021*\"movie\" + 0.016*\"shawshank\" + 0.015*\"prison\" + 0.011*\"andy\" + 0.011*\"redemption\"')\n",
            "(5, '0.023*\"film\" + 0.012*\"shawshank\" + 0.011*\"andy\" + 0.011*\"time\" + 0.009*\"best\"')\n",
            "(6, '0.001*\"film\" + 0.001*\"shawshank\" + 0.001*\"best\" + 0.001*\"prison\" + 0.001*\"redemption\"')\n",
            "(7, '0.022*\"film\" + 0.017*\"shawshank\" + 0.013*\"best\" + 0.010*\"andy\" + 0.010*\"prison\"')\n",
            "(8, '0.028*\"shawshank\" + 0.014*\"film\" + 0.009*\"andy\" + 0.009*\"just\" + 0.009*\"time\"')\n",
            "(9, '0.032*\"film\" + 0.015*\"best\" + 0.014*\"shawshank\" + 0.011*\"like\" + 0.010*\"king\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Load your dataset from assignment three\n",
        "# Replace 'your_dataset.csv' with the actual path or filename\n",
        "# Assuming the dataset has a column 'text' for the text data and 'label' for sentiment labels\n",
        "data = pd.read_csv('/content/Sentimental_analysed_dataset.csv')\n",
        "\n",
        "# Create a train-test split (80% for training, 20% for testing)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Features used for sentiment classification\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = tfidf_vectorizer.fit_transform(train_data['Review'])\n",
        "X_test = tfidf_vectorizer.transform(test_data['Review'])\n",
        "y_train = train_data['sentiment']\n",
        "y_test = test_data['sentiment']\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# Cross-validation using accuracy as the scoring metric\n",
        "rf_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "\n",
        "# Random Forest Performance Metrics\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X_train, y_train, cv=5)\n",
        "rf_accuracy = accuracy_score(y_train, rf_predictions)\n",
        "rf_precision = precision_score(y_train, rf_predictions, average='weighted')\n",
        "rf_recall = recall_score(y_train, rf_predictions, average='weighted')\n",
        "rf_f1 = f1_score(y_train, rf_predictions, average='weighted')\n",
        "\n",
        "# Display results\n",
        "\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "print(\"Precision:\", rf_precision)\n",
        "print(\"Recall:\", rf_recall)\n",
        "print(\"F1 Score:\", rf_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9akFv3_mA-1j",
        "outputId": "d25b6d42-2d7b-48c3-e858-ff0a58b81d5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ee06b0-bbcf-4700-ea88-fc8b45f0d308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1460 entries, 0 to 1459\n",
            "Data columns (total 81 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Id             1460 non-null   int64  \n",
            " 1   MSSubClass     1460 non-null   int64  \n",
            " 2   MSZoning       1460 non-null   object \n",
            " 3   LotFrontage    1201 non-null   float64\n",
            " 4   LotArea        1460 non-null   int64  \n",
            " 5   Street         1460 non-null   object \n",
            " 6   Alley          91 non-null     object \n",
            " 7   LotShape       1460 non-null   object \n",
            " 8   LandContour    1460 non-null   object \n",
            " 9   Utilities      1460 non-null   object \n",
            " 10  LotConfig      1460 non-null   object \n",
            " 11  LandSlope      1460 non-null   object \n",
            " 12  Neighborhood   1460 non-null   object \n",
            " 13  Condition1     1460 non-null   object \n",
            " 14  Condition2     1460 non-null   object \n",
            " 15  BldgType       1460 non-null   object \n",
            " 16  HouseStyle     1460 non-null   object \n",
            " 17  OverallQual    1460 non-null   int64  \n",
            " 18  OverallCond    1460 non-null   int64  \n",
            " 19  YearBuilt      1460 non-null   int64  \n",
            " 20  YearRemodAdd   1460 non-null   int64  \n",
            " 21  RoofStyle      1460 non-null   object \n",
            " 22  RoofMatl       1460 non-null   object \n",
            " 23  Exterior1st    1460 non-null   object \n",
            " 24  Exterior2nd    1460 non-null   object \n",
            " 25  MasVnrType     1452 non-null   object \n",
            " 26  MasVnrArea     1452 non-null   float64\n",
            " 27  ExterQual      1460 non-null   object \n",
            " 28  ExterCond      1460 non-null   object \n",
            " 29  Foundation     1460 non-null   object \n",
            " 30  BsmtQual       1423 non-null   object \n",
            " 31  BsmtCond       1423 non-null   object \n",
            " 32  BsmtExposure   1422 non-null   object \n",
            " 33  BsmtFinType1   1423 non-null   object \n",
            " 34  BsmtFinSF1     1460 non-null   int64  \n",
            " 35  BsmtFinType2   1422 non-null   object \n",
            " 36  BsmtFinSF2     1460 non-null   int64  \n",
            " 37  BsmtUnfSF      1460 non-null   int64  \n",
            " 38  TotalBsmtSF    1460 non-null   int64  \n",
            " 39  Heating        1460 non-null   object \n",
            " 40  HeatingQC      1460 non-null   object \n",
            " 41  CentralAir     1460 non-null   object \n",
            " 42  Electrical     1459 non-null   object \n",
            " 43  1stFlrSF       1460 non-null   int64  \n",
            " 44  2ndFlrSF       1460 non-null   int64  \n",
            " 45  LowQualFinSF   1460 non-null   int64  \n",
            " 46  GrLivArea      1460 non-null   int64  \n",
            " 47  BsmtFullBath   1460 non-null   int64  \n",
            " 48  BsmtHalfBath   1460 non-null   int64  \n",
            " 49  FullBath       1460 non-null   int64  \n",
            " 50  HalfBath       1460 non-null   int64  \n",
            " 51  BedroomAbvGr   1460 non-null   int64  \n",
            " 52  KitchenAbvGr   1460 non-null   int64  \n",
            " 53  KitchenQual    1460 non-null   object \n",
            " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
            " 55  Functional     1460 non-null   object \n",
            " 56  Fireplaces     1460 non-null   int64  \n",
            " 57  FireplaceQu    770 non-null    object \n",
            " 58  GarageType     1379 non-null   object \n",
            " 59  GarageYrBlt    1379 non-null   float64\n",
            " 60  GarageFinish   1379 non-null   object \n",
            " 61  GarageCars     1460 non-null   int64  \n",
            " 62  GarageArea     1460 non-null   int64  \n",
            " 63  GarageQual     1379 non-null   object \n",
            " 64  GarageCond     1379 non-null   object \n",
            " 65  PavedDrive     1460 non-null   object \n",
            " 66  WoodDeckSF     1460 non-null   int64  \n",
            " 67  OpenPorchSF    1460 non-null   int64  \n",
            " 68  EnclosedPorch  1460 non-null   int64  \n",
            " 69  3SsnPorch      1460 non-null   int64  \n",
            " 70  ScreenPorch    1460 non-null   int64  \n",
            " 71  PoolArea       1460 non-null   int64  \n",
            " 72  PoolQC         7 non-null      object \n",
            " 73  Fence          281 non-null    object \n",
            " 74  MiscFeature    54 non-null     object \n",
            " 75  MiscVal        1460 non-null   int64  \n",
            " 76  MoSold         1460 non-null   int64  \n",
            " 77  YrSold         1460 non-null   int64  \n",
            " 78  SaleType       1460 non-null   object \n",
            " 79  SaleCondition  1460 non-null   object \n",
            " 80  SalePrice      1460 non-null   int64  \n",
            "dtypes: float64(3), int64(35), object(43)\n",
            "memory usage: 924.0+ KB\n",
            "None\n",
            "Mean Squared Error: 6.56523054066197e-21\n",
            "R-squared: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# Assuming 'data' is your DataFrame with all columns including string and numeric ones\n",
        "data = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(data.info())\n",
        "\n",
        "# Separate numeric features (X) and target variable (y)\n",
        "numeric_features = data.select_dtypes(include=[np.number]).columns\n",
        "X = data[numeric_features]\n",
        "y = data['SalePrice']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data preprocessing - Impute missing values and standardize the features\n",
        "numeric_imputer = SimpleImputer(strategy='mean')\n",
        "X_train = numeric_imputer.fit_transform(X_train)\n",
        "X_test = numeric_imputer.transform(X_test)\n",
        "\n",
        "# Standardize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **pre-trained Large Language Model (LLM) from the Hugging Face Repository** for your specific task using the data collected in Assignment 3. After creating an account on Hugging Face (https://huggingface.co/), choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any Meta based text analysis model. Provide a brief description of the selected LLM, including its original sources, significant parameters, and any task-specific fine-tuning if applied.\n",
        "\n",
        "Perform a detailed analysis of the LLM's performance on your task, including key metrics, strengths, and limitations. Additionally, discuss any challenges encountered during the implementation and potential strategies for improvement. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As of my last knowledge update in January 2023, GPT-3, BERT, and RoBERTa are distinct models designed for different purposes. GPT-3 is a powerful generative language model developed by OpenAI, while BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa (Robustly optimized BERT approach) are specifically designed for tasks like natural language understanding. Each model has its strengths and weaknesses, and the choice depends on the specific requirements of your task.\n",
        "\n",
        "Given your request to analyze a Large Language Model (LLM) from the Hugging Face Repository, I'll provide a general overview of how you might approach this task. Please note that specific implementations might vary based on the model chosen.\n",
        "\n",
        "Example using BERT:\n",
        "Selecting a Model from Hugging Face:\n",
        "\n",
        "Go to the Hugging Face Model Hub (https://huggingface.co/models).\n",
        "Choose a pre-trained BERT model, such as bert-base-uncased or any other variant depending on your task.\n",
        "Brief Description:\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT is designed for natural language understanding. It considers the entire context of a word by looking at both left and right context words.\n",
        "Significant Parameters:\n",
        "\n",
        "BERT has various hyperparameters, but some of the essential ones include the learning rate, batch size, and the number of training epochs.\n",
        "The model architecture itself is quite complex, with multiple layers and attention mechanisms.\n",
        "Task-Specific Fine-Tuning:\n",
        "\n",
        "Depending on your specific task (e.g., sentiment analysis, text classification), you may need to fine-tune the pre-trained BERT model on your dataset.\n",
        "Fine-tuning involves training the model on your task-specific data to adapt it to your particular use case.\n",
        "Performance Analysis:\n",
        "Metrics:\n",
        "\n",
        "Common metrics for text classification tasks include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC).\n",
        "For text generation tasks, you might evaluate based on perplexity or BLEU score.\n",
        "Strengths:\n",
        "\n",
        "BERT is known for its contextual understanding, making it suitable for various NLP tasks.\n",
        "Pre-trained models from Hugging Face are easily accessible and can be fine-tuned for specific tasks with relatively little data.\n",
        "Limitations:\n",
        "\n",
        "BERT can be computationally expensive and memory-intensive.\n",
        "Fine-tuning may require a substantial amount of labeled data for specific tasks.\n",
        "Challenges and Strategies for Improvement:\n",
        "\n",
        "Data Quality: Ensure your dataset is representative and of high quality.\n",
        "Computational Resources: Address any computational constraints, such as GPU availability.\n",
        "Fine-tuning: Experiment with various hyperparameters during fine-tuning to optimize model performance.\n",
        "Conclusion:\n",
        "Selecting the appropriate model depends on your specific task and dataset. Fine-tuning a pre-trained LLM can yield impressive results, but it requires careful consideration of model architecture, hyperparameters, and evaluation metrics. Regularly checking the Hugging Face Model Hub for new models and improvements can also be beneficial."
      ],
      "metadata": {
        "id": "7o3E7kO19rPs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}