{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7557fc-31cb-40bc-bea2-37222bb2a263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to imdb_movie_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to scrape IMDB user reviews\n",
        "def scrape_imdb_reviews(movie_url):\n",
        "    headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.87 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(movie_url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Modify this part to extract the user reviews\n",
        "        review_divs = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        reviews = []\n",
        "        for review_div in review_divs:\n",
        "            review_text = review_div.get_text(strip=True)\n",
        "            reviews.append({'Review': review_text})\n",
        "\n",
        "        return reviews\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to retrieve the webpage.\")\n",
        "        return None\n",
        "\n",
        "# Example IMDB movie URL\n",
        "movie_url = \"https://www.imdb.com/title/tt0111161/reviews\"\n",
        "\n",
        "# Scrape the user reviews\n",
        "reviews_data = scrape_imdb_reviews(movie_url)\n",
        "\n",
        "if reviews_data:\n",
        "    # Save the data to a CSV file\n",
        "    df = pd.DataFrame(reviews_data)\n",
        "    df.to_csv('imdb_movie_reviews.csv', index=False)\n",
        "    print(\"Data saved to imdb_movie_reviews.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53dd8530-411e-4409-cd36-42e148fd93b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data with cleaned reviews saved to imdb_movie_reviews_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK data if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    # Remove punctuation and special characters\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "    # Remove numbers\n",
        "    text = ''.join([char for char in text if not char.isdigit()])\n",
        "\n",
        "    # Lowercase all text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Load the CSV file with the collected text data\n",
        "df = pd.read_csv('imdb_movie_reviews.csv')  # Replace with your actual CSV file\n",
        "\n",
        "# Clean the 'Review' column and store it in a new column 'Cleaned_Review'\n",
        "df['Cleaned_Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "# Save the data with cleaned text to a new CSV file\n",
        "df.to_csv('imdb_movie_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Data with cleaned reviews saved to imdb_movie_reviews_cleaned.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03474460-69e9-4682-b9a8-20a9d2f33ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech (POS) Counts:\n",
            "Counter({'PROPN': 7, 'VERB': 4, 'ADP': 3, 'PUNCT': 3, 'NOUN': 2, 'NUM': 2, 'PRON': 1, 'CCONJ': 1, 'DET': 1, 'AUX': 1})\n",
            "\n",
            "Dependency Parsing Trees:\n",
            "works\n",
            "lives\n",
            "scheduled\n",
            "\n",
            "Named Entities:\n",
            "PERSON: John\n",
            "ORG: Apple Inc.\n",
            "GPE: New York\n",
            "DATE: March 25, 2023\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function for syntax and structure analysis\n",
        "def analyze_text(text):\n",
        "    # (1) Parts of Speech (POS) Tagging and counting\n",
        "    doc = nlp(text)\n",
        "    pos_counts = Counter(token.pos_ for token in doc)\n",
        "\n",
        "    # (2) Constituency Parsing and Dependency Parsing using spaCy\n",
        "    dependency_trees = [sent.root for sent in doc.sents]\n",
        "\n",
        "    # (3) Named Entity Recognition (NER)\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    return pos_counts, dependency_trees, named_entities\n",
        "\n",
        "# Example sentence for analysis\n",
        "example_sentence = \"John works at Apple Inc. He lives in New York and loves Python programming. The meeting is scheduled for March 25, 2023.\"\n",
        "\n",
        "# Analyze the example sentence\n",
        "pos_counts, dependency_trees, named_entities = analyze_text(example_sentence)\n",
        "\n",
        "# Print the results\n",
        "print(\"Parts of Speech (POS) Counts:\")\n",
        "print(pos_counts)\n",
        "\n",
        "print(\"\\nDependency Parsing Trees:\")\n",
        "for tree in dependency_trees:\n",
        "    print(tree)\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for entity, label in named_entities:\n",
        "    print(f\"{label}: {entity}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constituency Parsing Tree:**\n",
        "\n",
        "Constituency parsing, also known as phrase structure parsing, is a natural language processing technique that aims to analyze the grammatical structure of a sentence. It involves breaking down a sentence into its constituent phrases or subparts. The primary elements in constituency parsing are:\n",
        "\n",
        "**Root Node:** At the top of the tree is the root node, which represents the entire sentence.\n",
        "Non-Terminal Nodes: These nodes represent phrases or subparts of the sentence, such as noun phrases (NP) or verb phrases (VP). Non-terminal nodes have children that are other non-terminal nodes or terminal nodes (words).\n",
        "\n",
        "**Terminal Nodes:** These nodes represent individual words in the sentence.\n",
        "\n",
        "Constituency parsing provides a hierarchical representation of the sentence's grammatical structure. For example, consider the sentence \"The quick brown fox jumps over the lazy dog.\" In a constituency parsing tree, you might have a structure like:\n",
        "\n",
        "(S\n",
        "  (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n",
        "  (VP (VBZ jumps) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog)))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this tree, \"S\" represents the entire sentence, \"NP\" represents noun phrases, \"VP\" represents verb phrases, and so on. It shows how the sentence is organized into different parts and how those parts relate to each other.\n",
        "\n",
        "**Dependency Parsing Tree:**\n",
        "\n",
        "Dependency parsing is another method for analyzing the grammatical structure of a sentence, but it focuses on the relationships between words in a sentence. In a dependency parsing tree:\n",
        "\n",
        "Each word in the sentence is a node in the tree.\n",
        "The relationships between words are represented by directed edges between the nodes.\n",
        "One word is typically the root of the tree, and all other words depend on it directly or indirectly.\n",
        "For example, in the sentence \"She eats pizza,\" the dependency parsing tree might look like this:\n",
        "\n",
        "eats ─► She\n",
        "\n",
        "eats ─► pizza\n",
        "\n",
        "In this tree, \"eats\" is the root, and \"She\" and \"pizza\" depend on \"eats\" as its subject and object, respectively.\n",
        "\n",
        "Dependency parsing is particularly useful for understanding the grammatical relationships in a sentence, such as subject-verb-object relationships, and it's often used in tasks like information extraction and named entity recognition.\n",
        "\n",
        "Both constituency parsing and dependency parsing provide valuable insights into the grammatical structure of sentences, and the choice between them depends on the specific needs of a natural language processing task."
      ],
      "metadata": {
        "id": "w5aHXWSk0MdK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}