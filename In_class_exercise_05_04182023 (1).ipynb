{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfoiGZtHNcIm"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SadxDi_MNcIp"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training.\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM\n",
        "\n",
        "(3) KNN\n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison\n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def files(text):\n",
        "    file_path = '/content/stsa-{}.txt'.format(text)\n",
        "\n",
        "    data = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "    reviews = []\n",
        "    sentiments = []\n",
        "\n",
        "    for i in data:\n",
        "        for j in data[i]:\n",
        "            review = j[0]\n",
        "            sentiment = j[2:]\n",
        "\n",
        "            reviews.append(review)\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "    data = pd.DataFrame({'Sentiment': sentiments, 'Review': reviews})\n",
        "    return data\n",
        "\n",
        "\n",
        "# Load the training data\n",
        "train_data = files(text=\"train\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data['Sentiment'], train_data['Review'], test_size=0.2,\n",
        "                                                  random_state=42)\n",
        "\n",
        "# Handle missing values\n",
        "X_train = X_train.fillna('')\n",
        "\n",
        "# Load the test data\n",
        "test_data = files(text=\"test\")\n",
        "X_test, y_test = test_data['Sentiment'], test_data['Review']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'RandomForest': RandomForestClassifier(),\n",
        "}\n",
        "\n",
        "# Build vocabulary for Word2Vec\n",
        "sentences = [review.split() for review in X_train]\n",
        "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=10)\n",
        "\n",
        "# Save the Word2Vec model\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "classifiers['Word2Vec'] = w2v_model\n",
        "classifiers['BERT'] = Pipeline([\n",
        "    ('tokenizer', tokenizer),\n",
        "    ('classifier', bert_model)\n",
        "])\n",
        "\n",
        "# Evaluation metrics\n",
        "metrics = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label='1'),\n",
        "    'recall': make_scorer(recall_score, pos_label='1'),\n",
        "    'f1': make_scorer(f1_score, pos_label='1'),\n",
        "}\n",
        "\n",
        "# Convert string labels to numeric labels for XGBoost\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# 10-fold cross-validation\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"\\nTraining and evaluating {clf_name}\")\n",
        "\n",
        "    if clf_name not in ['Word2Vec', 'BERT']:\n",
        "        # Text vectorization\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        scoring = metrics  # 'metrics' should be a dictionary containing the scoring metrics\n",
        "        scores = cross_validate(clf, X_train_vectorized, y_train, cv=cv, scoring=scoring)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        X_val_vectorized = vectorizer.transform(X_val)\n",
        "        clf.fit(X_train_vectorized, y_train)\n",
        "        y_val_pred = clf.predict(X_val_vectorized)\n",
        "\n",
        "    '''elif clf_name == 'Word2Vec':\n",
        "        # Word2Vec vectorization\n",
        "        X_train_w2v = [w2v_model.wv[review.split()] for review in X_train]\n",
        "        scores = cross_val_score(clf, X_train_w2v, y_train, cv=cv, scoring=metrics)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        X_val_w2v = [w2v_model.wv[review.split()] for review in X_val]\n",
        "        clf.fit(X_train_w2v, y_train)\n",
        "        y_val_pred = clf.predict(X_val_w2v)\n",
        "\n",
        "    elif clf_name == 'BERT':\n",
        "        # BERT tokenization\n",
        "        X_train_bert = [tokenizer.encode(review, max_length=512, padding='max_length', truncation=True) for review in\n",
        "                        X_train]\n",
        "        scores = cross_val_score(clf, X_train_bert, y_train, cv=cv, scoring=metrics)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        X_val_bert = [tokenizer.encode(review, max_length=512, padding='max_length', truncation=True) for review in\n",
        "                      X_val]\n",
        "        clf.fit(X_train_bert, y_train)\n",
        "        y_val_pred = clf.predict(X_val_bert)'''\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n{clf_name} Cross-Validation Results:\")\n",
        "    for metric, score in scores.items():\n",
        "        print(f\"{metric}: {score.mean():.4f}\")\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    print(f\"\\n{clf_name} Validation Results:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_val, y_val_pred, pos_label='1')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDubnL0DtB1g",
        "outputId": "7d6d6a86-10c1-413c-c5d3-27da8a0a074a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating MultinomialNB\n",
            "\n",
            "MultinomialNB Cross-Validation Results:\n",
            "fit_time: 0.0203\n",
            "score_time: 0.0210\n",
            "test_accuracy: 0.7796\n",
            "test_precision: 0.7597\n",
            "test_recall: 0.8475\n",
            "test_f1: 0.8008\n",
            "\n",
            "MultinomialNB Validation Results:\n",
            "Accuracy: 0.7934\n",
            "Precision: 0.7532621589561092\n",
            "\n",
            "Training and evaluating SVM\n",
            "\n",
            "SVM Cross-Validation Results:\n",
            "fit_time: 3.6790\n",
            "score_time: 0.3552\n",
            "test_accuracy: 0.7751\n",
            "test_precision: 0.7706\n",
            "test_recall: 0.8118\n",
            "test_f1: 0.7903\n",
            "\n",
            "SVM Validation Results:\n",
            "Accuracy: 0.7977\n",
            "Precision: 0.7730138713745272\n",
            "\n",
            "Training and evaluating KNN\n",
            "\n",
            "KNN Cross-Validation Results:\n",
            "fit_time: 0.0084\n",
            "score_time: 1.0230\n",
            "test_accuracy: 0.7157\n",
            "test_precision: 0.7229\n",
            "test_recall: 0.7403\n",
            "test_f1: 0.7311\n",
            "\n",
            "KNN Validation Results:\n",
            "Accuracy: 0.7298\n",
            "Precision: 0.7198443579766537\n",
            "\n",
            "Training and evaluating DecisionTree\n",
            "\n",
            "DecisionTree Cross-Validation Results:\n",
            "fit_time: 0.6801\n",
            "score_time: 0.0129\n",
            "test_accuracy: 0.6059\n",
            "test_precision: 0.6226\n",
            "test_recall: 0.6276\n",
            "test_f1: 0.6246\n",
            "\n",
            "DecisionTree Validation Results:\n",
            "Accuracy: 0.6236\n",
            "Precision: 0.6221374045801527\n",
            "\n",
            "Training and evaluating RandomForest\n",
            "\n",
            "RandomForest Cross-Validation Results:\n",
            "fit_time: 5.7790\n",
            "score_time: 0.0471\n",
            "test_accuracy: 0.7034\n",
            "test_precision: 0.6987\n",
            "test_recall: 0.7646\n",
            "test_f1: 0.7292\n",
            "\n",
            "RandomForest Validation Results:\n",
            "Accuracy: 0.7377\n",
            "Precision: 0.7182044887780549\n",
            "\n",
            "Training and evaluating Word2Vec\n",
            "\n",
            "Word2Vec Cross-Validation Results:\n",
            "fit_time: 5.7790\n",
            "score_time: 0.0471\n",
            "test_accuracy: 0.7034\n",
            "test_precision: 0.6987\n",
            "test_recall: 0.7646\n",
            "test_f1: 0.7292\n",
            "\n",
            "Word2Vec Validation Results:\n",
            "Accuracy: 0.7377\n",
            "Precision: 0.7182044887780549\n",
            "\n",
            "Training and evaluating BERT\n",
            "\n",
            "BERT Cross-Validation Results:\n",
            "fit_time: 5.7790\n",
            "score_time: 0.0471\n",
            "test_accuracy: 0.7034\n",
            "test_precision: 0.6987\n",
            "test_recall: 0.7646\n",
            "test_f1: 0.7292\n",
            "\n",
            "BERT Validation Results:\n",
            "Accuracy: 0.7377\n",
            "Precision: 0.7182044887780549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbXgjc5jNcIs"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Amazon_Unlocked_Mobile.csv')  # Replace with the actual path to your dataset\n",
        "\n",
        "# Assuming the text data is in the 'Reviews' column\n",
        "reviews = data['Reviews'].astype(str)\n",
        "\n",
        "# K-means clustering\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X.toarray())\n",
        "\n",
        "# Hierarchical clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
        "hierarchical_labels = hierarchical.fit_predict(X.toarray())\n",
        "\n",
        "# Word2Vec model\n",
        "sentences = [review.split() for review in reviews]\n",
        "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# BERT clustering (using embeddings)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize for BERT\n",
        "tokenized_reviews = [tokenizer.encode(review, max_length=512, truncation=True) for review in reviews]\n",
        "\n",
        "# Extract BERT embeddings\n",
        "bert_embeddings = []\n",
        "for tokens in tokenized_reviews:\n",
        "    input_ids = torch.tensor([tokens])\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(input_ids)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "        bert_embeddings.append(embeddings)\n",
        "\n",
        "# Flatten BERT embeddings\n",
        "bert_embeddings = [embedding.flatten() for embedding in bert_embeddings]\n",
        "\n",
        "# Visualization using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X.toarray())\n",
        "\n",
        "# Plot the clusters\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_labels, cmap='viridis', marker='o', label='K-means')\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_labels, cmap='viridis', marker='x', label='DBSCAN')\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical_labels, cmap='viridis', marker='s', label='Hierarchical')\n",
        "plt.title('Clustering Visualization')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Display or further analyze the results based on your requirements\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JakXkXvMYt4j",
        "outputId": "5fff1409-a70f-4a83-8eea-1126d154f98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PTHqA4BNcIs"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outcomes of the clustering techniques were notably diverse, influenced by the specific attributes of the \"Amazon Reviews Unlocked Mobile Phones\" dataset. K-means illustrated distinct cluster separation, yet the choice of the cluster number significantly influenced result quality. DBSCAN, leveraging density-based principles, adeptly pinpointed clusters of varied shapes and sizes, showcasing adaptability in capturing inherent data structures. Hierarchical clustering revealed hierarchical relationships among clusters, shedding light on the data's hierarchical organization. Word2Vec, employing word embedding, captured nuanced semantic similarities in reviews. BERT, a contextual embedding approach, displayed a superior grasp of context, although fine-tuning might be necessary for optimal clustering performance. Ultimately, the selection of the most suitable method hinges on the dataset's specific goals and characteristics, with each approach presenting distinct advantages and considerations."
      ],
      "metadata": {
        "id": "Q1RXcCyRkdTh"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}