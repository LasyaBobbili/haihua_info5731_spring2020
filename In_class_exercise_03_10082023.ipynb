{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "63b863ff-539a-4bb7-a086-ca40e8e2f1f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nBag of Words (BoW): This feature entails quantifying the frequency of individual words or tokens present in the text. It aids in capturing the most frequently used terms associated with either positive or negative sentiments. For instance, favorable comments may encompass words such as \"amazing,\" \"love,\" or \"great,\" while unfavorable comments may incorporate terms like \"disappointed,\" \"hate,\" or \"problem.\"\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF not only takes into account the occurrence of words within a single comment but also evaluates their significance within the entire dataset. It helps identify words that are specific to particular sentiments and may not commonly appear in general. For example, a high TF-IDF score for the word \"defective\" could suggest a negative sentiment.\\n\\nN-grams: N-grams capture sequences of words as opposed to individual words. Bi-grams (pairs of words) and tri-grams (groups of three words) assist the model in grasping the context in which words are utilized. For instance, the bi-gram \"customer service\" may signal a negative sentiment if frequently associated with complaints.\\n\\nEmotion Lexicons: Emotion lexicons comprise lists of words linked to specific emotions. By cross-referencing words in the text with these lexicons, the model can discern emotional nuances. For example, the presence of words like \"happy,\" \"joy,\" or \"excitement\" may indicate a positive sentiment.\\n\\nPart-of-Speech (POS) Tags: Scrutinizing the grammatical structure of a sentence using POS tags offers valuable insights. Positive sentiments often feature nouns such as \"product\" and adjectives like \"amazing,\" while negative sentiments might involve verbs like \"break\" and adjectives like \"terrible.\"\\n\\nSentiment Lexicons: Sentiment lexicons consist of words assigned known sentiment values (e.g., SentiWordNet). Assigning sentiment scores to words and aggregating them can help gauge the overall sentiment of a comment. For example, calculating the sum of positive scores minus the sum of negative scores can reveal sentiment polarity.\\n\\nPunctuation and Emoji Analysis: Examination of punctuation marks (e.g., exclamation points, question marks) and emojis can yield additional clues about sentiment. An excessive use of exclamation points may signify enthusiasm, while negative emojis like ðŸ˜¡ can indicate dissatisfaction.\\n\\nSentence Length: The length of a sentence can at times reflect sentiment. Short, succinct sentences often convey strong sentiments, while longer sentences may exhibit greater neutrality or analytical content.\\n\\nNamed Entity Recognition (NER): Identifying named entities (e.g., product names, company names) in the text allows for attributing sentiments to specific entities within a comment, offering more nuanced insights.\\n\\nContextual Word Embeddings: Utilizing pre-trained word embeddings like Word2Vec, GloVe, or BERT helps capture semantic relationships between words and phrases, enabling the model to grasp context more effectively.\\n\\nBy amalgamating these features, a machine learning model can proficiently categorize social media comments into positive, negative, or neutral sentiments, thereby furnishing valuable insights into the public\\'s perception of a novel product launch.\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Bag of Words (BoW): This feature entails quantifying the frequency of individual words or tokens present in the text. It aids in capturing the most frequently used terms associated with either positive or negative sentiments. For instance, favorable comments may encompass words such as \"amazing,\" \"love,\" or \"great,\" while unfavorable comments may incorporate terms like \"disappointed,\" \"hate,\" or \"problem.\"\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF not only takes into account the occurrence of words within a single comment but also evaluates their significance within the entire dataset. It helps identify words that are specific to particular sentiments and may not commonly appear in general. For example, a high TF-IDF score for the word \"defective\" could suggest a negative sentiment.\n",
        "\n",
        "N-grams: N-grams capture sequences of words as opposed to individual words. Bi-grams (pairs of words) and tri-grams (groups of three words) assist the model in grasping the context in which words are utilized. For instance, the bi-gram \"customer service\" may signal a negative sentiment if frequently associated with complaints.\n",
        "\n",
        "Emotion Lexicons: Emotion lexicons comprise lists of words linked to specific emotions. By cross-referencing words in the text with these lexicons, the model can discern emotional nuances. For example, the presence of words like \"happy,\" \"joy,\" or \"excitement\" may indicate a positive sentiment.\n",
        "\n",
        "Part-of-Speech (POS) Tags: Scrutinizing the grammatical structure of a sentence using POS tags offers valuable insights. Positive sentiments often feature nouns such as \"product\" and adjectives like \"amazing,\" while negative sentiments might involve verbs like \"break\" and adjectives like \"terrible.\"\n",
        "\n",
        "Sentiment Lexicons: Sentiment lexicons consist of words assigned known sentiment values (e.g., SentiWordNet). Assigning sentiment scores to words and aggregating them can help gauge the overall sentiment of a comment. For example, calculating the sum of positive scores minus the sum of negative scores can reveal sentiment polarity.\n",
        "\n",
        "Punctuation and Emoji Analysis: Examination of punctuation marks (e.g., exclamation points, question marks) and emojis can yield additional clues about sentiment. An excessive use of exclamation points may signify enthusiasm, while negative emojis like ðŸ˜¡ can indicate dissatisfaction.\n",
        "\n",
        "Sentence Length: The length of a sentence can at times reflect sentiment. Short, succinct sentences often convey strong sentiments, while longer sentences may exhibit greater neutrality or analytical content.\n",
        "\n",
        "Named Entity Recognition (NER): Identifying named entities (e.g., product names, company names) in the text allows for attributing sentiments to specific entities within a comment, offering more nuanced insights.\n",
        "\n",
        "Contextual Word Embeddings: Utilizing pre-trained word embeddings like Word2Vec, GloVe, or BERT helps capture semantic relationships between words and phrases, enabling the model to grasp context more effectively.\n",
        "\n",
        "By amalgamating these features, a machine learning model can proficiently categorize social media comments into positive, negative, or neutral sentiments, thereby furnishing valuable insights into the public's perception of a novel product launch.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d04e78c-8431-417d-bd2a-a55bb9aabc04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW): Counter({'.': 3, 'i': 2, 'absolutely': 1, 'love': 1, 'this': 1, 'product': 1, '!': 1, 'it': 1, \"'s\": 1, 'amazing': 1, 'and': 1, 'works': 1, 'great': 1, 'however': 1, ',': 1, 'the': 1, 'customer': 1, 'service': 1, 'was': 1, 'terrible': 1, \"'m\": 1, 'really': 1, 'disappointed': 1, 'ðŸ˜¡': 1})\n",
            "Bi-grams: ['i absolutely', 'absolutely love', 'love this', 'this product', 'product !', '! it', \"it 's\", \"'s amazing\", 'amazing and', 'and works', 'works great', 'great .', '. however', 'however ,', ', the', 'the customer', 'customer service', 'service was', 'was terrible', 'terrible .', '. i', \"i 'm\", \"'m really\", 'really disappointed', 'disappointed .', '. ðŸ˜¡']\n",
            "Emotion Lexicons: {'neg': 0.202, 'neu': 0.404, 'pos': 0.394, 'compound': 0.8016}\n",
            "POS Tags: ['PRP', 'RB', 'VBP', 'DT', 'NN', '.', 'PRP', 'VBZ', 'JJ', 'CC', 'VBZ', 'JJ', '.', 'RB', ',', 'DT', 'NN', 'NN', 'VBD', 'JJ', '.', 'PRP', 'VBP', 'RB', 'JJ', '.', 'NN']\n",
            "Sentence Lengths: [5, 5, 6, 3, 1]\n",
            "Named Entities: []\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import string\n",
        "import emoji\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Sample text data\n",
        "sample_text = \"I absolutely love this product! It's amazing and works great. However, the customer service was terrible. I'm really disappointed. ðŸ˜¡\"\n",
        "\n",
        "# Feature 1: Bag of Words (BoW)\n",
        "def bow_features(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return Counter(words)\n",
        "\n",
        "# Feature 2: TF-IDF (Term Frequency-Inverse Document Frequency) - Requires a document corpus\n",
        "# Not implemented here as it requires a larger dataset and a TF-IDF vectorizer\n",
        "\n",
        "# Feature 3: N-grams\n",
        "def ngram_features(text, n=2):\n",
        "    words = word_tokenize(text.lower())\n",
        "    ngrams_list = list(ngrams(words, n))\n",
        "    return [' '.join(ngram) for ngram in ngrams_list]\n",
        "\n",
        "# Feature 4: Emotion Lexicons\n",
        "def emotion_lexicon_features(text):\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Feature 5: Part-of-Speech (POS) Tags\n",
        "def pos_tag_features(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    return [tag for _, tag in pos_tags]\n",
        "\n",
        "# Feature 6: Sentiment Lexicons - Requires a lexicon (e.g., SentiWordNet)\n",
        "# Not implemented here as it requires a sentiment lexicon\n",
        "\n",
        "# Feature 7: Punctuation and Emoji Analysis\n",
        "'''def punctuation_emoji_features(text):\n",
        "    punctuation_count = Counter(char for char in text if char in string.punctuation)\n",
        "    emoji_count = len(emoji.emoji_count(text))\n",
        "\n",
        "    features = {\n",
        "        'punctuation_count': punctuation_count,\n",
        "        'emoji_count': emoji_count\n",
        "    }\n",
        "\n",
        "    return features'''\n",
        "\n",
        "# Feature 8: Sentence Length\n",
        "def sentence_length_feature(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return [len(sentence.split()) for sentence in sentences]\n",
        "\n",
        "# Feature 9: Named Entity Recognition (NER)\n",
        "def ner_features(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged = pos_tag(words)\n",
        "    named_entities = ne_chunk(tagged)\n",
        "    return [chunk for chunk in named_entities if hasattr(chunk, 'label')]\n",
        "\n",
        "# Feature 10: Contextual Word Embeddings - Requires spaCy or other word embedding model\n",
        "# Not implemented here as it requires a pre-trained model\n",
        "\n",
        "# Sample feature extraction\n",
        "bow = bow_features(sample_text)\n",
        "ngrams_2 = ngram_features(sample_text, n=2)\n",
        "emotion_lexicon = emotion_lexicon_features(sample_text)\n",
        "pos_tags = pos_tag_features(sample_text)\n",
        "'''punctuation, emojis = punctuation_emoji_features(sample_text)'''\n",
        "sentence_lengths = sentence_length_feature(sample_text)\n",
        "named_entities = ner_features(sample_text)\n",
        "\n",
        "print(\"Bag of Words (BoW):\", bow)\n",
        "print(\"Bi-grams:\", ngrams_2)\n",
        "print(\"Emotion Lexicons:\", emotion_lexicon)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "'''print(\"Punctuation Count:\", punctuation)\n",
        "print(\"Emoji Count:\", emojis)'''\n",
        "print(\"Sentence Lengths:\", sentence_lengths)\n",
        "print(\"Named Entities:\", named_entities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa8a91f-c21e-437d-be37-97154f10b265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absolutely: 0.24253562503633297\n",
            "amazing: 0.24253562503633297\n",
            "and: 0.24253562503633297\n",
            "customer: 0.24253562503633297\n",
            "disappointed: 0.24253562503633297\n",
            "great: 0.24253562503633297\n",
            "however: 0.24253562503633297\n",
            "it: 0.24253562503633297\n",
            "love: 0.24253562503633297\n",
            "product: 0.24253562503633297\n",
            "really: 0.24253562503633297\n",
            "service: 0.24253562503633297\n",
            "terrible: 0.24253562503633297\n",
            "the: 0.24253562503633297\n",
            "this: 0.24253562503633297\n",
            "was: 0.24253562503633297\n",
            "works: 0.24253562503633297\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "sample_text = \"I absolutely love this product! It's amazing and works great. However, the customer service was terrible. I'm really disappointed. ðŸ˜¡\"\n",
        "\n",
        "# Create a list of texts (only one text in this case)\n",
        "texts = [sample_text]\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Get the feature names (words or n-grams)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the TF-IDF scores for each feature\n",
        "tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "# Create a dictionary to store feature names and their TF-IDF scores\n",
        "feature_tfidf_dict = dict(zip(feature_names, tfidf_scores))\n",
        "\n",
        "# Rank features based on their TF-IDF scores in descending order\n",
        "sorted_features = sorted(feature_tfidf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted features\n",
        "for feature, tfidf_score in sorted_features:\n",
        "    print(f\"{feature}: {tfidf_score}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6e5257-36b1-4b6e-dd1b-c6aa64eaadc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: I'm looking for a great product. Can you recommend one?\n",
            "Rank 1: Similarity Score: 0.7520\n",
            "I absolutely love this product! It's amazing and works great.\n",
            "\n",
            "Rank 2: Similarity Score: 0.7400\n",
            "The customer service was terrible. I'm really disappointed.\n",
            "\n",
            "Rank 3: Similarity Score: 0.6768\n",
            "This product is just average, nothing special.\n",
            "\n",
            "Rank 4: Similarity Score: 0.6190\n",
            "I have never seen such a terrible product before.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Sample text data\n",
        "text_data = [\n",
        "    \"I absolutely love this product! It's amazing and works great.\",\n",
        "    \"The customer service was terrible. I'm really disappointed.\",\n",
        "    \"This product is just average, nothing special.\",\n",
        "    \"I have never seen such a terrible product before.\",\n",
        "]\n",
        "\n",
        "# Query to match against the text data\n",
        "query = \"I'm looking for a great product. Can you recommend one?\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Encode the query and text data\n",
        "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "text_tokens = tokenizer(text_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Compute BERT embeddings for the query and text data\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**query_tokens).last_hidden_state.mean(dim=1)\n",
        "    text_embeddings = model(**text_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "cosine_similarities = cosine_similarity(query_embedding, text_embeddings).flatten()\n",
        "\n",
        "# Rank documents based on similarity in descending order\n",
        "sorted_indices = np.argsort(cosine_similarities)[::-1]\n",
        "sorted_text_data = [text_data[i] for i in sorted_indices]\n",
        "sorted_similarity_scores = [cosine_similarities[i] for i in sorted_indices]\n",
        "\n",
        "# Print ranked results\n",
        "print(\"Query:\", query)\n",
        "for i, (text, score) in enumerate(zip(sorted_text_data, sorted_similarity_scores), start=1):\n",
        "    print(f\"Rank {i}: Similarity Score: {score:.4f}\\n{text}\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEe4ulWSjbsX"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}